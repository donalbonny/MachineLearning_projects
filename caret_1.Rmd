---
title: "MAchine_learning_first_test"
author: "Vinh Pham"
date: "January 25, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Machine learning using R
How do you start machine learning in R


```{r}
library(caret)
	
```

Load the data 

```{r}
data("iris")
dataset <- iris
filename <- "iris.csv"

# load the CSV file from the local directory
dataset <- read.csv(filename, header=FALSE)
```

```{r}
colnames(dataset) <- c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species")
head(dataset)
```
```{r}
dim(dataset)
```
We will split the loaded dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset.


```{r}
validation_index <- createDataPartition(dataset$Species, p = 0.8, list = FALSE)
validation <- dataset[-validation_index,]


#use the remaining 80% of data to training and testing the models
dataset <-  dataset[validation_index,]


# Check the dimesion of dataset after split the validation
dim(dataset)
```

```{r}

```

```{r}
sapply(dataset, class)
```

```{r}
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
```

```{r}
#statistical summary of dataset
summary(dataset)
```
We are going to look at two types of plots:

Univariate plots to better understand each attribute.

Multivariate plots to better understand the relationships between attributes.
```{r}
#split input and output

x <- dataset[,1:4]
y <- dataset[,5]
```

```{r}
par(mfrow = c(1,4))
for (i in 1:4)
{
  boxplot(x[,i], main = names(iris)[i])
}

```

```{r}
plot(y)
```



```{r}
featurePlot(x=x, y=y, plot="box")
```

```{r}
#run algorithm using 10 fold cross validation. Use 10-fold crossvalidation to estimate accuracy.
control <- trainControl(method = 'cv', number = 10)
metric <- "Accuracy"

```
#Build the model
```{r}


set.seed(2018)
nrow(dataset)
fit.lda <- train(Species~., data = dataset, method = "lda", metric = metric, trControl = control)
#nonlinear algorithms
set.seed(2018)
fit.cart <- train(Species~., data = dataset, method = "rpart", metric = metric, trControl = control)
#kNN
set.seed(2018)
fit.knn <- train(Species~., data = dataset, method = "knn", metric = metric, trControl = control)
# c) advanced algorithms
set.seed(2018)
fit.svm <-  train( Species~., data = dataset, method = "svmRadial", metric = metric, trControl = control)
#random forest 
set.seed(2018)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)

```
#Select Best Model
```{r}
result <-  resamples(list(lda = fit.lda, cart = fit.cart, knn = fit.knn, svm = fit.svm, rf = fit.rf))
summary(result)
```

```{r}
dotplot(result)
```
We can see that the most accurate model in this case was LDA

The result of LDA model can be summarised:

```{r}
#you can see the best model is LDA


print(fit.lda)
```
This gives a nice summary of what was used to train the model and the mean and standard deviation (SD) accuracy achieved, specifically 97.5% accuracy +/- 4%


#Make prediction

Now we want to get an idea of the accuracy of the model on our validation set.

```{r}
#make prediction using LDA model

prediction <-  predict(fit.lda, validation)
confusionMatrix(prediction, validation$Species)

```
We can see that the accuracy is 100%. It was a small validation dataset (20%), but this result is within our expected margin of 97% +/-4% suggesting we may have an accurate and a reliably accurate model.

